{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03050374-58d4-4388-a334-d06a395bfd40",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart Foundation Models - HuggingFace Text2Text Instruction Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5383e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b27d1-cd77-46d7-88f0-68748156703d",
   "metadata": {},
   "source": [
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use SageMaker JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart).\n",
    "\n",
    "In this demo notebook, we use the SageMaker Python SDK to **fine-tune a Text2Text model**. Such a model takes prompting text as input and generates text as output. The prompt can include a task description in natural language. Accordingly, the model can be used for a variety of NLP tasks (e.g., text summarization, question answering, etc.).\n",
    "\n",
    "We will fine-tune a pre-trained **FLAN T5 model** from [Hugging Face](https://huggingface.co/docs/transformers/model_doc/flan-t5). While pre-trained FLAN T5 models can be used \"as is\" for many tasks, fine-tuning can improve model performance on a particular task or language domain. As an example, we will fine-tune the model for a task that was not used for pre-training. After fine-tuning we will deploy two inference endpoints, one with a pre-trained and one with a fine-tuned model. We will then run the same inference query against both endpoints and compare results.\n",
    "\n",
    "*This notebook was tested on ml.t3.medium Amazon SageMaker Notebook instance with conda_python3 kernel.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1706e56-3d74-4f2c-b5cd-695acca57d5c",
   "metadata": {},
   "source": [
    "#### In this notebook:\n",
    "1. [Setting up](#1.-Setting-up)\n",
    "1. [Fine-tuning a model](#2.-Fine-tuning-a-model)\n",
    "1. [Deploying inference endpoints](#3.-Deploying-inference-endpoints)\n",
    "1. [Running inference queries](#4.-Running-inference-queries)\n",
    "1. [Cleaning up resources](#5.-Cleaning-up-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30630a-685b-4be1-9c32-e50252a77b87",
   "metadata": {},
   "source": [
    "### 1. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97ab4c-f05c-4696-8040-f7b0a17ba21e",
   "metadata": {},
   "source": [
    "We begin by installing and upgrading necessary packages. Restart the kernel after executing the cell below for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9bf59d-f7fb-47e1-b2cf-7be6a819be96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==2.166.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a91ac-431d-4adc-8285-4e1dd73721d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use the following variables throughout the notebook. In particular, we select FLAN T5 model size and select training and inference instance types. We also obtain execution role associated with the current notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c83140fe-f5d2-49ee-a433-70109cf23bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1maws_region:\u001b[0m us-east-1\n",
      "\u001b[1maws_role:\u001b[0m arn:aws:iam::722831609225:role/service-role/AmazonSageMaker-ExecutionRole-20230401T004358\n",
      "\u001b[1moutput_bucket:\u001b[0m sagemaker-us-east-1-722831609225\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Get current region, role, and default bucket\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_role = sagemaker.session.Session().get_caller_identity_arn()\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "# This will be useful for printing\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "print(f\"{bold}aws_region:{unbold} {aws_region}\")\n",
    "print(f\"{bold}aws_role:{unbold} {aws_role}\")\n",
    "print(f\"{bold}output_bucket:{unbold} {output_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013de37d-0681-44e1-acce-0a854a5b6492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# Replace with larger model if needed\n",
    "model_id, model_version = \"huggingface-text2text-flan-t5-base\", \"*\"\n",
    "pretrained_model = JumpStartModel(model_id=model_id)\n",
    "#pretrained_predictor = pretrained_model.deploy()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcebc7-f7ac-4a9b-882b-2f83dca59186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''print(pretrained_model.endpoint_name)\n",
    "pre_trained_name = pretrained_model.endpoint_name'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a5c9b6b-7e38-4ad5-a757-4e202f9171be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''text = \"\"\"Summarize this content - Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. \n",
    "You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. \"\"\"\n",
    "query_response = pretrained_predictor.predict(text)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59aa544a-368b-4fd5-9c31-0740a34041ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understand the benefits of Amazon Comprehend. Understand the benefits of Amazon Comprehend\n"
     ]
    }
   ],
   "source": [
    "'''print(query_response[\"generated_text\"])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "421b7c0c-3a68-45dc-891b-d3ff2631f25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''from sagemaker import serializers\n",
    "from sagemaker import content_types\n",
    "\n",
    "\n",
    "serializer_options = serializers.retrieve_options(model_id=model_id, model_version=model_version)\n",
    "content_type_options = content_types.retrieve_options(\n",
    "    model_id=model_id, model_version=model_version\n",
    ")\n",
    "\n",
    "pretrained_predictor.serializer = serializers.JSONSerializer()\n",
    "pretrained_predictor.content_type = \"application/json\" '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1846ed32-7bbf-425f-aabb-9e898b12700f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understand the benefits of Amazon Comprehend. Understand the benefits of Amazon Comprehend.\n"
     ]
    }
   ],
   "source": [
    "'''from sagemaker import serializers\n",
    "\n",
    "input_text = \"\"\"Summarize this content - Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases.\n",
    "You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. \"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 600,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_p\": 0.01,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "payload = {\"text_inputs\": input_text, **parameters}  # JSON Input format\n",
    "query_response = pretrained_predictor.predict(payload)\n",
    "print(query_response[\"generated_texts\"][0])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32db7d6-b63f-447e-b2ca-16fa4c620956",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Delete resources to free up instances\n",
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c7754-ffc3-40a0-9f85-bda3b25161d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Fine-tuning a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d898c437-0b3a-47b2-9b25-827d824f83ec",
   "metadata": {},
   "source": [
    "FLAN T5 models were pre-trained on a variety of tasks. In this demo, we fine-tune a model for a new task. In this task, given a piece of text, the model is asked to generate questions that are relevant to the text, but cannot be answered based on provided information. Examples are given in the inference section of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c377423-c7d5-4087-a175-717227d47936",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1. Preparing training data\n",
    "We will use a subset of SQuAD2.0 for supervised fine-tuning. This dataset contains questions posed by human annotators on a set of Wikipedia articles. In addition to questions with answers, SQuAD2.0 contains about 50k unanswerable questions. Such questions are plausible, but cannot be directly answered from the articles' content. We only use unanswerable questions for our task.\n",
    "\n",
    "*Citation: @article{rajpurkar2018know, title={Know what you don't know: Unanswerable questions for SQuAD},\n",
    "author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy}, journal={arXiv preprint arXiv:1806.03822}, year={2018} }*\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96bb8fb1-84e1-43d3-981b-769856e1c204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train-v2.0.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# We will use the train split of SQuAD2.0\n",
    "original_data_file = \"train-v2.0.json\"\n",
    "\n",
    "# The data was mirrored in the following bucket\n",
    "original_data_location = (\n",
    "    f\"s3://sagemaker-example-files-prod-{aws_region}/datasets/text/squad2.0/{original_data_file}\"\n",
    ")\n",
    "S3Downloader.download(original_data_location, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c3efd-63ae-4d01-9b13-cd9c48c1af92",
   "metadata": {},
   "source": [
    "The Text2Text generation model can be fine-tuned on any text data provided that the data is in the expected format. The data must include a training and an optional validation parts. The best model is selected according to the validation loss, calculated at the end of each epoch. If a validation set is not given, an (adjustable) percentage of the training data is automatically split and used for validation.\n",
    "\n",
    "The training data must be formatted in JSON lines (`.jsonl`) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training folder can also contain a `template.json` file describing the input and output formats.\n",
    "\n",
    "If no template file is given, the following default template will be used:\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"{prompt}\",\n",
    "    \"completion\": \"{completion}\"\n",
    "}\n",
    "```\n",
    "In this case, the data in the JSON lines entries must include `prompt` and `completion` fields.\n",
    "\n",
    "In this demo, we are going to use a custom template (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3002bc68-2779-4fc0-a68b-cbdd0b3663bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "local_data_file = \"task-data.jsonl\"  # any name with .jsonl extension\n",
    "\n",
    "with open(original_data_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(local_data_file, \"w\") as f:\n",
    "    for article in data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            # iterate over questions for a given paragraph\n",
    "            for qas in paragraph[\"qas\"]:\n",
    "                if qas[\"is_impossible\"]:\n",
    "                    # the question is relevant, but cannot be answered\n",
    "                    example = {\"context\": paragraph[\"context\"], \"question\": qas[\"question\"]}\n",
    "                    json.dump(example, f)\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Ask a question which is related to the following text, but cannot be answered based on the text. Text: {context}\",\n",
    "    \"completion\": \"{question}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc435de-a9c3-403e-a24a-f5ef032755cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtraining data:\u001b[0m s3://sagemaker-us-east-1-722831609225/train_data\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "train_data_location = f\"s3://{output_bucket}/train_data\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"{bold}training data:{unbold} {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069eebf-0799-4e99-baf8-8a0201a01600",
   "metadata": {},
   "source": [
    "We will use simplified JumpStart SDK that defaults various input paramters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf3377-4cd7-40df-876b-e0fa2f57dc8f",
   "metadata": {},
   "source": [
    "#### 2.2. Starting training\n",
    "\n",
    "We are now ready to start the training job. This can take a while to complete, from 20 minutes to several hours, depending on the model size, amount of data, and so on (e.g., it can take a few hours for the `xl` model, 40k examples and 3 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: hf-text2text-flan-t5-base-2023-10-20-03-35-25-789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-20 03:35:26 Starting - Starting the training job...\n",
      "2023-10-20 03:35:55 Starting - Preparing the instances for training.........\n",
      "2023-10-20 03:37:08 Downloading - Downloading input data.........\n",
      "2023-10-20 03:38:28 Training - Downloading the training image...........................\n",
      "2023-10-20 03:43:19 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:39,533 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:39,554 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:39,567 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:39,570 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:40,209 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.19.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/deepspeed/deepspeed-0.9.2.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.4-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.0.2-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 3)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 3)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.3.0->-r requirements.txt (line 4)) (4.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0->-r requirements.txt (line 2)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.3.0->-r requirements.txt (line 4)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.3.0->-r requirements.txt (line 4)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.12.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.9.2-py3-none-any.whl size=811224 sha256=ccc6f2faf5a96fb70c6f34540399b73069f75457ea518cfd9ac0ec259a3977e5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/31/4e/6c/adcb68c67e187eb0fb3939cc0ed6357effa4660dbd006f5fa4\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, deepspeed, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.16.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.16.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.16.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.9.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.9.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.9.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.19.0 datasets-2.12.0 deepspeed-0.9.2 peft-0.3.0 sagemaker-jumpstart-huggingface-script-utilities-1.0.2 sagemaker-jumpstart-script-utilities-1.1.4 sagemaker-jumpstart-tabular-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:56,288 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:56,288 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:56,328 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:56,363 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:56,398 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-20 03:43:56,425 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.999\",\n",
      "        \"adam_epsilon\": \"1e-08\",\n",
      "        \"auto_find_batch_size\": \"False\",\n",
      "        \"batch_size\": \"64\",\n",
      "        \"dataloader_drop_last\": \"False\",\n",
      "        \"dataloader_num_workers\": \"0\",\n",
      "        \"early_stopping_patience\": \"3\",\n",
      "        \"early_stopping_threshold\": \"0.0\",\n",
      "        \"epoch\": \"3\",\n",
      "        \"epochs\": \"1\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_steps\": \"500\",\n",
      "        \"evalaution_strategy\": \"epoch\",\n",
      "        \"gradient_accumulation_steps\": \"1\",\n",
      "        \"gradient_checkpointing\": \"True\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"label_smoothing_factor\": \"0\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"load_best_model_at_end\": \"True\",\n",
      "        \"logging_first_step\": \"False\",\n",
      "        \"logging_nan_inf_filter\": \"True\",\n",
      "        \"logging_steps\": \"500\",\n",
      "        \"logging_strategy\": \"steps\",\n",
      "        \"lr_scheduler_type\": \"constant_with_warmup\",\n",
      "        \"max_eval_samples\": \"-1\",\n",
      "        \"max_grad_norm\": \"1.0\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_output_length\": \"128\",\n",
      "        \"max_steps\": \"-1\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"pad_to_max_length\": \"True\",\n",
      "        \"peft_type\": \"none\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"save_steps\": \"500\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": \"2\",\n",
      "        \"seed\": \"42\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.05\",\n",
      "        \"warmup_ratio\": \"0.0\",\n",
      "        \"warmup_steps\": \"0\",\n",
      "        \"weight_decay\": \"0.0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"hf-text2text-flan-t5-base-2023-10-20-03-35-25-789\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"batch_size\":\"64\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"3\",\"epochs\":\"1\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"500\",\"evalaution_strategy\":\"epoch\",\"gradient_accumulation_steps\":\"1\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"True\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"0.0001\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"500\",\"logging_strategy\":\"steps\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_eval_samples\":\"-1\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"1024\",\"max_output_length\":\"128\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"pad_to_max_length\":\"True\",\"peft_type\":\"none\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"epoch\",\"save_total_limit\":\"2\",\"seed\":\"42\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.05\",\"warmup_ratio\":\"0.0\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"batch_size\":\"64\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"3\",\"epochs\":\"1\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"500\",\"evalaution_strategy\":\"epoch\",\"gradient_accumulation_steps\":\"1\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"True\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"0.0001\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"500\",\"logging_strategy\":\"steps\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_eval_samples\":\"-1\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"1024\",\"max_output_length\":\"128\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"pad_to_max_length\":\"True\",\"peft_type\":\"none\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"epoch\",\"save_total_limit\":\"2\",\"seed\":\"42\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.05\",\"warmup_ratio\":\"0.0\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"hf-text2text-flan-t5-base-2023-10-20-03-35-25-789\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.999\",\"--adam_epsilon\",\"1e-08\",\"--auto_find_batch_size\",\"False\",\"--batch_size\",\"64\",\"--dataloader_drop_last\",\"False\",\"--dataloader_num_workers\",\"0\",\"--early_stopping_patience\",\"3\",\"--early_stopping_threshold\",\"0.0\",\"--epoch\",\"3\",\"--epochs\",\"1\",\"--eval_accumulation_steps\",\"None\",\"--eval_steps\",\"500\",\"--evalaution_strategy\",\"epoch\",\"--gradient_accumulation_steps\",\"1\",\"--gradient_checkpointing\",\"True\",\"--instruction_tuned\",\"True\",\"--label_smoothing_factor\",\"0\",\"--learning_rate\",\"0.0001\",\"--load_best_model_at_end\",\"True\",\"--logging_first_step\",\"False\",\"--logging_nan_inf_filter\",\"True\",\"--logging_steps\",\"500\",\"--logging_strategy\",\"steps\",\"--lr_scheduler_type\",\"constant_with_warmup\",\"--max_eval_samples\",\"-1\",\"--max_grad_norm\",\"1.0\",\"--max_input_length\",\"1024\",\"--max_output_length\",\"128\",\"--max_steps\",\"-1\",\"--max_train_samples\",\"-1\",\"--pad_to_max_length\",\"True\",\"--peft_type\",\"none\",\"--preprocessing_num_workers\",\"None\",\"--save_steps\",\"500\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"2\",\"--seed\",\"42\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.05\",\"--warmup_ratio\",\"0.0\",\"--warmup_steps\",\"0\",\"--weight_decay\",\"0.0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_EPSILON=1e-08\u001b[0m\n",
      "\u001b[34mSM_HP_AUTO_FIND_BATCH_SIZE=False\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_DROP_LAST=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_NUM_WORKERS=0\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_THRESHOLD=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=3\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_EVALAUTION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=1\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=True\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_SMOOTHING_FACTOR=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FIRST_STEP=False\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_NAN_INF_FILTER=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant_with_warmup\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_EVAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_OUTPUT_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=True\u001b[0m\n",
      "\u001b[34mSM_HP_PEFT_TYPE=none\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=2\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --auto_find_batch_size False --batch_size 64 --dataloader_drop_last False --dataloader_num_workers 0 --early_stopping_patience 3 --early_stopping_threshold 0.0 --epoch 3 --epochs 1 --eval_accumulation_steps None --eval_steps 500 --evalaution_strategy epoch --gradient_accumulation_steps 1 --gradient_checkpointing True --instruction_tuned True --label_smoothing_factor 0 --learning_rate 0.0001 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 500 --logging_strategy steps --lr_scheduler_type constant_with_warmup --max_eval_samples -1 --max_grad_norm 1.0 --max_input_length 1024 --max_output_length 128 --max_steps -1 --max_train_samples -1 --pad_to_max_length True --peft_type none --preprocessing_num_workers None --save_steps 500 --save_strategy epoch --save_total_limit 2 --seed 42 --train_data_split_seed 0 --validation_split_ratio 0.05 --warmup_ratio 0.0 --warmup_steps 0 --weight_decay 0.0\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:44:02.029: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-10-20 03:44:02,035 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-10-20 03:44:02,063 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mExtracting model to /tmp/extracted\u001b[0m\n",
      "\u001b[34mINFO:__main__:Extracting model to /tmp/extracted\u001b[0m\n",
      "\u001b[34mFound existing /opt/ml/model. This directory will be emptied.\u001b[0m\n",
      "\u001b[34mWARNING:__main__:Found existing /opt/ml/model. This directory will be emptied.\u001b[0m\n",
      "\u001b[34mError deleting /opt/ml/model: (<class 'OSError'>, OSError(16, 'Device or resource busy'), <traceback object at 0x7fb9c9134180>)\u001b[0m\n",
      "\u001b[34mWARNING:__main__:Error deleting /opt/ml/model: (<class 'OSError'>, OSError(16, 'Device or resource busy'), <traceback object at 0x7fb9c9134180>)\u001b[0m\n",
      "\u001b[34mINFO:root:Preparing training data...\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-76820a95914dd65a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 4471.54it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1106.38it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 43498 examples [00:00, 373674.25 examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-76820a95914dd65a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mNo validation data provided, splitting automatically.\u001b[0m\n",
      "\u001b[34mINFO:data_preprocessor:No validation data provided, splitting automatically.\u001b[0m\n",
      "\u001b[34mWARNING:root:The max_input_length passed (1024) is larger than the maximum length for themodel (512). Using max_input_length=512.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/41323 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 1000/41323 [00:01<00:41, 973.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 2000/41323 [00:02<00:39, 997.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 3000/41323 [00:03<00:38, 992.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 4000/41323 [00:04<00:37, 985.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 5000/41323 [00:05<00:36, 1001.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 6000/41323 [00:06<00:35, 1006.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 7000/41323 [00:07<00:35, 980.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 8000/41323 [00:08<00:33, 992.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 9000/41323 [00:09<00:32, 997.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 10000/41323 [00:10<00:31, 997.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 11000/41323 [00:11<00:30, 1002.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 12000/41323 [00:12<00:29, 1006.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███▏      | 13000/41323 [00:13<00:28, 999.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 14000/41323 [00:14<00:27, 1008.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▋      | 15000/41323 [00:14<00:25, 1012.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 16000/41323 [00:15<00:24, 1016.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 17000/41323 [00:16<00:23, 1016.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▎     | 18000/41323 [00:17<00:23, 999.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 19000/41323 [00:19<00:22, 979.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 20000/41323 [00:20<00:21, 990.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 21000/41323 [00:21<00:20, 997.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 22000/41323 [00:21<00:19, 1007.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 23000/41323 [00:22<00:18, 1010.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 24000/41323 [00:23<00:17, 1011.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 25000/41323 [00:24<00:16, 1001.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 26000/41323 [00:25<00:15, 1002.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 27000/41323 [00:26<00:14, 998.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 28000/41323 [00:27<00:13, 1003.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 29000/41323 [00:28<00:12, 1001.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 30000/41323 [00:29<00:11, 1003.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 31000/41323 [00:31<00:10, 984.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 32000/41323 [00:32<00:09, 995.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 33000/41323 [00:33<00:08, 999.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 34000/41323 [00:34<00:07, 996.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▍ | 35000/41323 [00:34<00:06, 1002.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 36000/41323 [00:35<00:05, 1010.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 37000/41323 [00:36<00:04, 1014.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 38000/41323 [00:37<00:03, 1016.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 39000/41323 [00:38<00:02, 1015.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 40000/41323 [00:39<00:01, 1017.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 41000/41323 [00:40<00:00, 1020.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 41323/41323 [00:41<00:00, 1015.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2175 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 1000/2175 [00:01<00:01, 944.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 2000/2175 [00:02<00:00, 979.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2175/2175 [00:02<00:00, 977.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):   0%|          | 0/41323 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):  77%|███████▋  | 32000/41323 [00:00<00:00, 310403.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 41323/41323 [00:00<00:00, 310403.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):   0%|          | 0/2175 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 2175/2175 [00:00<00:00, 218872.63 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking DeepSpeed with command ['deepspeed', '--num_gpus', '1', 'train.py', '--model_dir', '/opt/ml/model', '--pretrained_model_dir', '/tmp/extracted', '--generation_max_length', '128', '--peft_type', 'none', '--adam_beta1', '0.9', '--adam_beta2', '0.999', '--adam_epsilon', '1e-08', '--auto_find_batch_size', 'False', '--batch_size', '64', '--dataloader_drop_last', 'False', '--dataloader_num_workers', '0', '--early_stopping_patience', '3', '--early_stopping_threshold', '0.0', '--epoch', '3', '--epochs', '1', '--eval_accumulation_steps', 'None', '--eval_steps', '500', '--evalaution_strategy', 'epoch', '--gradient_accumulation_steps', '1', '--gradient_checkpointing', 'True', '--instruction_tuned', 'True', '--label_smoothing_factor', '0', '--learning_rate', '0.0001', '--load_best_model_at_end', 'True', '--logging_first_step', 'False', '--logging_nan_inf_filter', 'True', '--logging_steps', '500', '--logging_strategy', 'steps', '--lr_scheduler_type', 'constant_with_warmup', '--max_grad_norm', '1.0', '--max_steps', '-1', '--save_steps', '500', '--save_strategy', 'epoch', '--save_total_limit', '2', '--seed', '42', '--warmup_ratio', '0.0', '--warmup_steps', '0', '--weight_decay', '0.0']\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:04,263] [WARNING] [runner.py:191:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:04,279] [INFO] [runner.py:541:main] cmd = /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --model_dir /opt/ml/model --pretrained_model_dir /tmp/extracted --generation_max_length 128 --peft_type none --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --auto_find_batch_size False --batch_size 64 --dataloader_drop_last False --dataloader_num_workers 0 --early_stopping_patience 3 --early_stopping_threshold 0.0 --epoch 3 --epochs 1 --eval_accumulation_steps None --eval_steps 500 --evalaution_strategy epoch --gradient_accumulation_steps 1 --gradient_checkpointing True --instruction_tuned True --label_smoothing_factor 0 --learning_rate 0.0001 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 500 --logging_strategy steps --lr_scheduler_type constant_with_warmup --max_grad_norm 1.0 --max_steps -1 --save_steps 500 --save_strategy epoch --save_total_limit 2 --seed 42 --warmup_ratio 0.0 --warmup_steps 0 --weight_decay 0.0\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:07,562] [INFO] [launch.py:222:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:07,562] [INFO] [launch.py:222:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:07,562] [INFO] [launch.py:222:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:07,562] [INFO] [launch.py:222:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:07,562] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0]}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:07,562] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:07,562] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:07,562] [INFO] [launch.py:247:main] dist_world_size=1\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:07,563] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(pretrained_model_dir='/tmp/extracted', output_data_dir='/opt/ml/output/data', checkpoint_dir='/tmp', model_dir='/opt/ml/model', epochs=1, max_steps=-1, generation_max_length=128, generation_num_beams=4, learning_rate=0.0001, batch_size=64, seed=42, bf16='auto', offload='auto', num_gpus=1, gradient_checkpointing='True', load_best_model_at_end='True', early_stopping_patience=3, early_stopping_threshold=0.0, gradient_accumulation_steps=1, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0.0, logging_strategy='steps', logging_first_step='False', logging_steps=500, logging_nan_inf_filter='True', save_strategy='epoch', save_steps=500, save_total_limit='2', dataloader_drop_last='False', dataloader_num_workers=0, evaluation_strategy='epoch', eval_steps=500, eval_accumulation_steps=None, auto_find_batch_size='False', lr_scheduler_type='constant_with_warmup', warmup_ratio=0.0, warmup_steps=0, peft_type='none', local_rank=0).\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--epoch', '3', '--evalaution_strategy', 'epoch', '--instruction_tuned', 'True'].\u001b[0m\n",
      "\u001b[34mINFO:root:Model class: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\u001b[0m\n",
      "\u001b[34mINFO:root:DeepSpeed config: DeepSpeedConfig(bf16=False, offload=False, zero_opt_overwrites=None, global_overwrites=None, ds_lr_scheduler='WarmupLR').\u001b[0m\n",
      "\u001b[34mINFO:root:Collecting training args...\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:16,586] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\u001b[0m\n",
      "\u001b[34mINFO:root:Training...\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:16,635] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34malgo-1:196:229 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:196:229 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:18,262] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 34.88281989097595 seconds\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:53,906] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:53,923] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:53,924] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:53,924] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:53,924] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:54,036] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:54,037] [INFO] [utils.py:786:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.99 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:54,037] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.71 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:54,041] [INFO] [stage3.py:113:__init__] Reduce bucket size 589824\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:45:54,041] [INFO] [stage3.py:114:__init__] Prefetch bucket size 530841\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 21.94476294517517 seconds\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:16,114] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:16,115] [INFO] [utils.py:786:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.99 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:16,115] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.71 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 48384 in 64 params\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:16,327] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:16,328] [INFO] [utils.py:786:see_memory_usage] MA 0.93 GB         Max_MA 1.02 GB         CA 1.18 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:16,328] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.71 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:16,434] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:16,435] [INFO] [utils.py:786:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 1.18 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:16,436] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.71 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,642] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,643] [INFO] [utils.py:786:see_memory_usage] MA 0.92 GB         Max_MA 0.93 GB         CA 0.93 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,643] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.75 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,731] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,732] [INFO] [utils.py:786:see_memory_usage] MA 0.92 GB         Max_MA 0.92 GB         CA 0.93 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,732] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.75 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,826] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,827] [INFO] [utils.py:786:see_memory_usage] MA 1.84 GB         Max_MA 1.84 GB         CA 1.85 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,827] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.75 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,917] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,917] [INFO] [utils.py:786:see_memory_usage] MA 1.84 GB         Max_MA 1.84 GB         CA 1.85 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:17,918] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.75 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,013] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,014] [INFO] [utils.py:786:see_memory_usage] MA 3.69 GB         Max_MA 4.61 GB         CA 4.62 GB         Max_CA 5 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,014] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.75 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,015] [INFO] [stage3.py:392:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,225] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,226] [INFO] [utils.py:786:see_memory_usage] MA 4.61 GB         Max_MA 4.8 GB         CA 5.54 GB         Max_CA 6 GB\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,226] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.75 GB, percent = 7.9%\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,226] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,227] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,227] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7feb48d83f70>\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,228] [INFO] [config.py:955:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7feae6f55730>\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,229] [INFO] [config.py:959:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,230] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,231] [INFO] [config.py:959:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 0}\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,232] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   train_batch_size ............. 64\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  64\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   world_size ................... 1\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=589824 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=530841 param_persistence_threshold=7680 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:959:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18,233] [INFO] [config.py:945:print_user_config]   json = {\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.0001, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.0001, \n",
      "            \"warmup_num_steps\": 0\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 5.898240e+05, \n",
      "        \"stage3_prefetch_bucket_size\": 5.308416e+05, \n",
      "        \"stage3_param_persistence_threshold\": 7.680000e+03, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 64, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0006060600280761719 seconds\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 41323\u001b[0m\n",
      "\u001b[34mNum examples = 41323\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 646\u001b[0m\n",
      "\u001b[34mNum Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 646\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 0\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 0\u001b[0m\n",
      "\u001b[34m0%|          | 0/646 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18.355: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18.396 algo-1:196 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18.436 algo-1:196 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18.437 algo-1:196 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18.437 algo-1:196 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18.438 algo-1:196 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-20 03:46:18.438 algo-1:196 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/646 [00:05<58:57,  5.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/646 [00:15<1:26:06,  8.02s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/646 [00:19<1:06:58,  6.25s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/646 [00:23<57:54,  5.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/646 [00:27<52:53,  4.95s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/646 [00:31<49:51,  4.67s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/646 [00:35<47:54,  4.50s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 8/646 [00:40<46:36,  4.38s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 9/646 [00:44<45:46,  4.31s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10/646 [00:48<45:09,  4.26s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 11/646 [00:52<44:41,  4.22s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 12/646 [00:56<44:20,  4.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 13/646 [01:00<44:04,  4.18s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/646 [01:04<43:53,  4.17s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/646 [01:09<43:46,  4.16s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 16/646 [01:13<43:40,  4.16s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 17/646 [01:17<43:33,  4.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 18/646 [01:21<43:26,  4.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 19/646 [01:25<43:21,  4.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/646 [01:29<43:16,  4.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 21/646 [01:33<43:12,  4.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 22/646 [01:38<43:10,  4.15s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 23/646 [01:42<43:06,  4.15s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 24/646 [01:46<43:02,  4.15s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 25/646 [01:50<42:59,  4.15s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 26/646 [01:54<42:55,  4.15s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 27/646 [01:58<42:51,  4.15s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 28/646 [02:03<42:48,  4.16s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 29/646 [02:07<42:44,  4.16s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 30/646 [02:11<42:42,  4.16s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 31/646 [02:15<42:37,  4.16s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 32/646 [02:19<42:34,  4.16s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 33/646 [02:23<42:30,  4.16s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 34/646 [02:28<42:27,  4.16s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 35/646 [02:32<42:23,  4.16s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 36/646 [02:36<42:19,  4.16s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 37/646 [02:40<42:15,  4.16s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 38/646 [02:44<42:13,  4.17s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 39/646 [02:48<42:12,  4.17s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 40/646 [02:53<42:09,  4.17s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 41/646 [02:57<42:05,  4.17s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 42/646 [03:01<42:02,  4.18s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 43/646 [03:05<41:57,  4.18s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 44/646 [03:09<41:53,  4.17s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 45/646 [03:13<41:50,  4.18s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 46/646 [03:18<41:47,  4.18s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 47/646 [03:22<41:43,  4.18s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 48/646 [03:26<41:38,  4.18s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 49/646 [03:30<41:36,  4.18s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 50/646 [03:34<41:33,  4.18s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 51/646 [03:39<41:29,  4.18s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 52/646 [03:43<41:24,  4.18s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 53/646 [03:47<41:20,  4.18s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 54/646 [03:51<41:16,  4.18s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 55/646 [03:55<41:11,  4.18s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 56/646 [03:59<41:07,  4.18s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 57/646 [04:04<41:03,  4.18s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 58/646 [04:08<40:59,  4.18s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 59/646 [04:12<40:55,  4.18s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 60/646 [04:16<40:53,  4.19s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 61/646 [04:20<40:49,  4.19s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 62/646 [04:25<40:45,  4.19s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 63/646 [04:29<40:40,  4.19s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 64/646 [04:33<40:35,  4.18s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 65/646 [04:37<40:30,  4.18s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 66/646 [04:41<40:27,  4.19s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 67/646 [04:46<40:22,  4.18s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 68/646 [04:50<40:19,  4.19s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 69/646 [04:54<40:15,  4.19s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 70/646 [04:58<40:11,  4.19s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 71/646 [05:02<40:08,  4.19s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 72/646 [05:06<40:04,  4.19s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 73/646 [05:11<40:00,  4.19s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 74/646 [05:15<39:56,  4.19s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 75/646 [05:19<39:51,  4.19s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 76/646 [05:23<39:47,  4.19s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 77/646 [05:27<39:42,  4.19s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 78/646 [05:32<39:38,  4.19s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 79/646 [05:36<39:34,  4.19s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 80/646 [05:40<39:30,  4.19s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 81/646 [05:44<39:26,  4.19s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 82/646 [05:48<39:21,  4.19s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 83/646 [05:53<39:17,  4.19s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 84/646 [05:57<39:13,  4.19s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 85/646 [06:01<39:10,  4.19s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 86/646 [06:05<39:05,  4.19s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 87/646 [06:09<39:00,  4.19s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 88/646 [06:13<38:55,  4.19s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 89/646 [06:18<38:51,  4.19s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 90/646 [06:22<38:48,  4.19s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 91/646 [06:26<38:43,  4.19s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 92/646 [06:30<38:39,  4.19s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 93/646 [06:34<38:35,  4.19s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 94/646 [06:39<38:31,  4.19s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 95/646 [06:43<38:27,  4.19s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 96/646 [06:47<38:23,  4.19s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 97/646 [06:51<38:19,  4.19s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 98/646 [06:55<38:14,  4.19s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 99/646 [07:00<38:10,  4.19s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 100/646 [07:04<38:04,  4.18s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 101/646 [07:08<38:00,  4.18s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 102/646 [07:12<37:56,  4.18s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 103/646 [07:16<37:53,  4.19s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 104/646 [07:20<37:48,  4.19s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 105/646 [07:25<37:43,  4.18s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 106/646 [07:29<37:38,  4.18s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 107/646 [07:33<37:33,  4.18s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 108/646 [07:37<37:29,  4.18s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 109/646 [07:41<37:25,  4.18s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 110/646 [07:46<37:21,  4.18s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 111/646 [07:50<37:17,  4.18s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 112/646 [07:54<37:13,  4.18s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 113/646 [07:58<37:09,  4.18s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 114/646 [08:02<37:05,  4.18s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 115/646 [08:06<37:01,  4.18s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 116/646 [08:11<36:56,  4.18s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 117/646 [08:15<36:52,  4.18s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 118/646 [08:19<36:47,  4.18s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 119/646 [08:23<36:43,  4.18s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 120/646 [08:27<36:38,  4.18s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 121/646 [08:32<36:35,  4.18s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 122/646 [08:36<36:32,  4.18s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 123/646 [08:40<36:27,  4.18s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 124/646 [08:44<36:22,  4.18s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 125/646 [08:48<36:19,  4.18s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 126/646 [08:52<36:15,  4.18s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 127/646 [08:57<36:11,  4.18s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 128/646 [09:01<36:06,  4.18s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 129/646 [09:05<36:02,  4.18s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 130/646 [09:09<35:58,  4.18s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 131/646 [09:13<35:54,  4.18s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 132/646 [09:18<35:50,  4.18s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 133/646 [09:22<35:46,  4.18s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 134/646 [09:26<35:42,  4.18s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 135/646 [09:30<35:37,  4.18s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 136/646 [09:34<35:32,  4.18s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 137/646 [09:38<35:28,  4.18s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 138/646 [09:43<35:25,  4.18s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 139/646 [09:47<35:22,  4.19s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 140/646 [09:51<35:17,  4.18s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 141/646 [09:55<35:12,  4.18s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 142/646 [09:59<35:08,  4.18s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 143/646 [10:04<35:03,  4.18s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 144/646 [10:08<34:59,  4.18s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 145/646 [10:12<34:55,  4.18s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 146/646 [10:16<34:51,  4.18s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 147/646 [10:20<34:46,  4.18s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 148/646 [10:24<34:41,  4.18s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 149/646 [10:29<34:37,  4.18s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 150/646 [10:33<34:33,  4.18s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 151/646 [10:37<34:29,  4.18s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 152/646 [10:41<34:25,  4.18s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 153/646 [10:45<34:22,  4.18s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 154/646 [10:50<34:17,  4.18s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 155/646 [10:54<34:13,  4.18s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 156/646 [10:58<34:09,  4.18s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 157/646 [11:02<34:05,  4.18s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 158/646 [11:06<34:02,  4.18s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 159/646 [11:10<33:57,  4.18s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 160/646 [11:15<33:52,  4.18s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 161/646 [11:19<33:48,  4.18s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 162/646 [11:23<33:44,  4.18s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 163/646 [11:27<33:39,  4.18s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 164/646 [11:31<33:34,  4.18s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 165/646 [11:36<33:31,  4.18s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 166/646 [11:40<33:28,  4.18s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 167/646 [11:44<33:22,  4.18s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 168/646 [11:48<33:17,  4.18s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 169/646 [11:52<33:14,  4.18s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 170/646 [11:56<33:10,  4.18s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 171/646 [12:01<33:06,  4.18s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 172/646 [12:05<33:02,  4.18s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 173/646 [12:09<32:58,  4.18s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 174/646 [12:13<32:53,  4.18s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 175/646 [12:17<32:49,  4.18s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 176/646 [12:22<32:45,  4.18s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 177/646 [12:26<32:41,  4.18s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 178/646 [12:30<32:37,  4.18s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 179/646 [12:34<32:33,  4.18s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 180/646 [12:38<32:29,  4.18s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 181/646 [12:42<32:25,  4.18s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 182/646 [12:47<32:21,  4.19s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 183/646 [12:51<32:18,  4.19s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 184/646 [12:55<32:14,  4.19s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 185/646 [12:59<32:11,  4.19s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 186/646 [13:03<32:06,  4.19s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 187/646 [13:08<32:02,  4.19s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 188/646 [13:12<31:57,  4.19s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 189/646 [13:16<31:53,  4.19s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 190/646 [13:20<31:48,  4.19s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 191/646 [13:24<31:43,  4.18s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 192/646 [13:29<31:40,  4.19s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 193/646 [13:33<31:36,  4.19s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 194/646 [13:37<31:31,  4.19s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 195/646 [13:41<31:27,  4.18s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 196/646 [13:45<31:23,  4.18s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 197/646 [13:49<31:18,  4.18s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 198/646 [13:54<31:15,  4.19s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 199/646 [13:58<31:11,  4.19s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 200/646 [14:02<31:06,  4.19s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 201/646 [14:06<31:01,  4.18s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 202/646 [14:10<30:57,  4.18s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 203/646 [14:15<30:52,  4.18s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 204/646 [14:19<30:47,  4.18s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 205/646 [14:23<30:42,  4.18s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 206/646 [14:27<30:39,  4.18s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 207/646 [14:31<30:36,  4.18s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 208/646 [14:35<30:32,  4.18s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 209/646 [14:40<30:28,  4.18s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 210/646 [14:44<30:23,  4.18s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 211/646 [14:48<30:19,  4.18s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 212/646 [14:52<30:16,  4.19s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 213/646 [14:56<30:12,  4.18s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 214/646 [15:01<30:07,  4.18s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 215/646 [15:05<30:02,  4.18s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 216/646 [15:09<29:58,  4.18s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 217/646 [15:13<29:54,  4.18s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 218/646 [15:17<29:49,  4.18s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 219/646 [15:21<29:45,  4.18s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 220/646 [15:26<29:41,  4.18s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 221/646 [15:30<29:37,  4.18s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 222/646 [15:34<29:33,  4.18s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 223/646 [15:38<29:28,  4.18s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 224/646 [15:42<29:24,  4.18s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 225/646 [15:47<29:20,  4.18s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 226/646 [15:51<29:17,  4.18s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 227/646 [15:55<29:14,  4.19s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 228/646 [15:59<29:09,  4.19s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 229/646 [16:03<29:04,  4.18s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 230/646 [16:07<29:00,  4.18s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 231/646 [16:12<28:55,  4.18s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 232/646 [16:16<28:51,  4.18s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 233/646 [16:20<28:46,  4.18s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 234/646 [16:24<28:42,  4.18s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 235/646 [16:28<28:37,  4.18s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 236/646 [16:33<28:33,  4.18s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 237/646 [16:37<28:30,  4.18s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 238/646 [16:41<28:25,  4.18s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 239/646 [16:45<28:22,  4.18s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 240/646 [16:49<28:18,  4.18s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 241/646 [16:53<28:14,  4.18s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 242/646 [16:58<28:10,  4.18s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 243/646 [17:02<28:05,  4.18s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 244/646 [17:06<28:01,  4.18s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 245/646 [17:10<27:57,  4.18s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 246/646 [17:14<27:53,  4.18s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 247/646 [17:19<27:50,  4.19s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 248/646 [17:23<27:44,  4.18s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 249/646 [17:27<27:41,  4.18s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 250/646 [17:31<27:36,  4.18s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 251/646 [17:35<27:32,  4.18s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 252/646 [17:40<27:29,  4.19s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 253/646 [17:44<27:24,  4.18s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 254/646 [17:48<27:19,  4.18s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 255/646 [17:52<27:15,  4.18s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 256/646 [17:56<27:12,  4.18s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 257/646 [18:00<27:08,  4.19s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 258/646 [18:05<27:03,  4.18s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 259/646 [18:09<26:59,  4.18s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 260/646 [18:13<26:54,  4.18s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 261/646 [18:17<26:50,  4.18s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 262/646 [18:21<26:46,  4.18s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 263/646 [18:26<26:43,  4.19s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 264/646 [18:30<26:37,  4.18s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 265/646 [18:34<26:33,  4.18s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 266/646 [18:38<26:30,  4.18s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 267/646 [18:42<26:26,  4.19s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 268/646 [18:46<26:22,  4.19s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 269/646 [18:51<26:18,  4.19s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 270/646 [18:55<26:13,  4.19s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 271/646 [18:59<26:10,  4.19s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 272/646 [19:03<26:06,  4.19s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 273/646 [19:07<26:02,  4.19s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 274/646 [19:12<25:57,  4.19s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 275/646 [19:16<25:53,  4.19s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 276/646 [19:20<25:48,  4.19s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 277/646 [19:24<25:44,  4.19s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 278/646 [19:28<25:39,  4.18s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 279/646 [19:33<25:36,  4.19s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 280/646 [19:37<25:31,  4.19s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 281/646 [19:41<25:28,  4.19s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 282/646 [19:45<25:23,  4.19s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 283/646 [19:49<25:20,  4.19s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 284/646 [19:53<25:15,  4.19s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 285/646 [19:58<25:10,  4.18s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 286/646 [20:02<25:06,  4.18s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 287/646 [20:06<25:02,  4.18s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 288/646 [20:10<24:57,  4.18s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 289/646 [20:14<24:52,  4.18s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 290/646 [20:19<24:48,  4.18s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 291/646 [20:23<24:44,  4.18s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 292/646 [20:27<24:40,  4.18s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 293/646 [20:31<24:36,  4.18s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 294/646 [20:35<24:32,  4.18s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 295/646 [20:39<24:28,  4.18s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 296/646 [20:44<24:24,  4.18s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 297/646 [20:48<24:19,  4.18s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 298/646 [20:52<24:14,  4.18s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 299/646 [20:56<24:10,  4.18s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 300/646 [21:00<24:06,  4.18s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 301/646 [21:05<24:03,  4.18s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 302/646 [21:09<23:58,  4.18s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 303/646 [21:13<23:55,  4.18s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 304/646 [21:17<23:51,  4.19s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 305/646 [21:21<23:46,  4.18s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 306/646 [21:25<23:42,  4.18s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 307/646 [21:30<23:39,  4.19s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 308/646 [21:34<23:35,  4.19s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 309/646 [21:38<23:30,  4.19s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 310/646 [21:42<23:26,  4.19s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 311/646 [21:46<23:22,  4.19s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 312/646 [21:51<23:18,  4.19s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 313/646 [21:55<23:14,  4.19s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 314/646 [21:59<23:09,  4.19s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 315/646 [22:03<23:04,  4.18s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 316/646 [22:07<23:00,  4.18s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 317/646 [22:12<22:55,  4.18s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 318/646 [22:16<22:51,  4.18s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 319/646 [22:20<22:47,  4.18s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 320/646 [22:24<22:44,  4.19s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 321/646 [22:28<22:39,  4.18s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 322/646 [22:32<22:35,  4.18s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 323/646 [22:37<22:30,  4.18s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 324/646 [22:41<22:26,  4.18s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 325/646 [22:45<22:21,  4.18s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 326/646 [22:49<22:18,  4.18s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 327/646 [22:53<22:13,  4.18s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 328/646 [22:58<22:10,  4.18s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 329/646 [23:02<22:05,  4.18s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 330/646 [23:06<22:02,  4.19s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 331/646 [23:10<21:58,  4.18s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 332/646 [23:14<21:53,  4.18s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 333/646 [23:18<21:48,  4.18s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 334/646 [23:23<21:45,  4.18s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 335/646 [23:27<21:41,  4.18s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 336/646 [23:31<21:36,  4.18s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 337/646 [23:35<21:32,  4.18s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 338/646 [23:39<21:28,  4.18s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 339/646 [23:44<21:23,  4.18s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 340/646 [23:48<21:19,  4.18s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 341/646 [23:52<21:14,  4.18s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 342/646 [23:56<21:10,  4.18s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 343/646 [24:00<21:06,  4.18s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 344/646 [24:04<21:02,  4.18s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 345/646 [24:09<20:58,  4.18s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 346/646 [24:13<20:54,  4.18s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 347/646 [24:17<20:50,  4.18s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 348/646 [24:21<20:46,  4.18s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 349/646 [24:25<20:41,  4.18s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 350/646 [24:30<20:38,  4.18s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 351/646 [24:34<20:34,  4.19s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 352/646 [24:38<20:30,  4.19s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 353/646 [24:42<20:26,  4.18s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 354/646 [24:46<20:21,  4.18s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 355/646 [24:50<20:17,  4.18s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 356/646 [24:55<20:13,  4.18s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 357/646 [24:59<20:09,  4.18s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 358/646 [25:03<20:05,  4.19s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 359/646 [25:07<20:00,  4.18s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 360/646 [25:11<19:57,  4.19s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 361/646 [25:16<19:53,  4.19s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 362/646 [25:20<19:49,  4.19s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 363/646 [25:24<19:44,  4.18s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 364/646 [25:28<19:39,  4.18s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 365/646 [25:32<19:34,  4.18s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 366/646 [25:36<19:31,  4.18s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 367/646 [25:41<19:26,  4.18s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 368/646 [25:45<19:22,  4.18s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 369/646 [25:49<19:19,  4.18s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 370/646 [25:53<19:14,  4.18s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 371/646 [25:57<19:10,  4.18s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 372/646 [26:02<19:06,  4.18s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 373/646 [26:06<19:01,  4.18s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 374/646 [26:10<18:57,  4.18s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 375/646 [26:14<18:53,  4.18s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 376/646 [26:18<18:48,  4.18s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 377/646 [26:22<18:45,  4.18s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 378/646 [26:27<18:41,  4.18s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 379/646 [26:31<18:37,  4.18s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 380/646 [26:35<18:32,  4.18s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 381/646 [26:39<18:28,  4.18s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 382/646 [26:43<18:24,  4.18s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 383/646 [26:48<18:20,  4.18s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 384/646 [26:52<18:16,  4.19s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 385/646 [26:56<18:12,  4.18s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 386/646 [27:00<18:07,  4.18s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 387/646 [27:04<18:03,  4.18s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 388/646 [27:09<17:59,  4.19s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 389/646 [27:13<17:56,  4.19s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 390/646 [27:17<17:51,  4.19s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 391/646 [27:21<17:46,  4.18s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 392/646 [27:25<17:42,  4.18s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 393/646 [27:29<17:37,  4.18s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 394/646 [27:34<17:33,  4.18s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 395/646 [27:38<17:29,  4.18s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 396/646 [27:42<17:25,  4.18s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 397/646 [27:46<17:21,  4.18s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 398/646 [27:50<17:17,  4.18s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 399/646 [27:55<17:13,  4.18s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 400/646 [27:59<17:08,  4.18s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 401/646 [28:03<17:03,  4.18s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 402/646 [28:07<16:59,  4.18s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 403/646 [28:11<16:55,  4.18s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 404/646 [28:15<16:51,  4.18s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 405/646 [28:20<16:47,  4.18s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 406/646 [28:24<16:43,  4.18s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 407/646 [28:28<16:38,  4.18s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 408/646 [28:32<16:34,  4.18s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 409/646 [28:36<16:30,  4.18s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 410/646 [28:40<16:26,  4.18s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 411/646 [28:45<16:22,  4.18s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 412/646 [28:49<16:18,  4.18s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 413/646 [28:53<16:14,  4.18s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 414/646 [28:57<16:10,  4.18s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 415/646 [29:01<16:06,  4.18s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 416/646 [29:06<16:02,  4.18s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 417/646 [29:10<15:57,  4.18s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 418/646 [29:14<15:53,  4.18s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 419/646 [29:18<15:49,  4.18s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 420/646 [29:22<15:45,  4.18s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 421/646 [29:26<15:40,  4.18s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 422/646 [29:31<15:36,  4.18s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 423/646 [29:35<15:32,  4.18s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 424/646 [29:39<15:28,  4.18s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 425/646 [29:43<15:23,  4.18s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 426/646 [29:47<15:19,  4.18s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 427/646 [29:52<15:15,  4.18s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 428/646 [29:56<15:11,  4.18s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 429/646 [30:00<15:07,  4.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 430/646 [30:04<15:02,  4.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 431/646 [30:08<14:59,  4.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 432/646 [30:12<14:55,  4.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 433/646 [30:17<14:50,  4.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 434/646 [30:21<14:46,  4.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 435/646 [30:25<14:42,  4.18s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 436/646 [30:29<14:38,  4.18s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 437/646 [30:33<14:34,  4.18s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 438/646 [30:38<14:30,  4.19s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 439/646 [30:42<14:26,  4.19s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 440/646 [30:46<14:22,  4.19s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 441/646 [30:50<14:18,  4.19s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 442/646 [30:54<14:14,  4.19s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 443/646 [30:59<14:10,  4.19s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 444/646 [31:03<14:06,  4.19s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 445/646 [31:07<14:02,  4.19s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 446/646 [31:11<13:58,  4.19s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 447/646 [31:15<13:53,  4.19s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 448/646 [31:20<13:49,  4.19s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 449/646 [31:24<13:45,  4.19s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 450/646 [31:28<13:41,  4.19s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 451/646 [31:32<13:36,  4.19s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 452/646 [31:36<13:32,  4.19s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 453/646 [31:40<13:28,  4.19s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 454/646 [31:45<13:24,  4.19s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 455/646 [31:49<13:19,  4.19s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 456/646 [31:53<13:15,  4.19s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 457/646 [31:57<13:11,  4.19s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 458/646 [32:01<13:07,  4.19s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 459/646 [32:06<13:03,  4.19s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 460/646 [32:10<12:59,  4.19s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 461/646 [32:14<12:55,  4.19s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 462/646 [32:18<12:50,  4.19s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 463/646 [32:22<12:46,  4.19s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 464/646 [32:27<12:42,  4.19s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 465/646 [32:31<12:38,  4.19s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 466/646 [32:35<12:33,  4.19s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 467/646 [32:39<12:29,  4.18s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 468/646 [32:43<12:25,  4.19s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 469/646 [32:47<12:20,  4.18s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 470/646 [32:52<12:16,  4.19s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 471/646 [32:56<12:13,  4.19s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 472/646 [33:00<12:08,  4.19s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 473/646 [33:04<12:04,  4.19s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 474/646 [33:08<12:00,  4.19s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 475/646 [33:13<11:56,  4.19s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 476/646 [33:17<11:51,  4.19s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 477/646 [33:21<11:47,  4.18s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 478/646 [33:25<11:42,  4.18s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 479/646 [33:29<11:38,  4.18s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 480/646 [33:33<11:34,  4.18s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 481/646 [33:38<11:30,  4.18s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 482/646 [33:42<11:25,  4.18s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 483/646 [33:46<11:21,  4.18s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 484/646 [33:50<11:17,  4.18s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 485/646 [33:54<11:13,  4.19s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id, instance_type=\"ml.p3.2xlarge\"\n",
    ")\n",
    "\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"3\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524dead-769d-4b5d-88b2-e9587dab474e",
   "metadata": {},
   "source": [
    "Performance metrics such as training and validation loss can be accessed through CloudWatch during training. We can also fetch the most recent snapshot of metrics as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ba317-699a-4b86-9cf4-254a3f5bdbe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Deploying inference endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629eaa2c-b415-4bd7-b8bc-500f118862e5",
   "metadata": {},
   "source": [
    "Remainder of the notebook should be executed once the training job is successfully completed. \n",
    "We will inference endpoints of pretrained and finetuned models and run the same request against the two endpoints and compare the results.\n",
    "Note that each endpoint deployment can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e607c9ea-efcf-4f90-89d7-71cf1f87ed93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: hf-text2text-flan-t5-base-2023-10-19-18-57-53-269\n",
      "INFO:sagemaker:Creating endpoint-config with name hf-text2text-flan-t5-base-2023-10-19-18-57-53-264\n",
      "INFO:sagemaker:Creating endpoint with name hf-text2text-flan-t5-base-2023-10-19-18-57-53-264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd359f69-7e48-4962-88f6-38a9e090b489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf-text2text-flan-t5-base-2023-10-19-18-57-53-264\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_name = finetuned_predictor.endpoint_name\n",
    "print(fine_tuned_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58114831-c08e-4542-bc5e-852826785356",
   "metadata": {},
   "source": [
    "### 4. Running inference queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ebf33c-dc6f-41a9-b137-1e2ffaa8de2e",
   "metadata": {},
   "source": [
    "As the name suggests, a Text2Text model such as FLAN T5 receives a piece of text as input, and generates text as output. The input text will contain the description of the task. In this demo, our task is to generate questions given a piece of text. The questions must be relevant to the text, but the text should contain no answer. Such a task could arise when automating gathering additional information, or identifying gaps in technical documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a0c4d9-dc5f-4fdd-9175-05f584ebbc73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does Amazon Comprehend use to create new products based on understanding the structure of documents?\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 40,  # restrict the length of the generated text\n",
    "    \"num_return_sequences\": 5,  # we will inspect several model outputs\n",
    "    \"num_beams\": 10,  # use beam search\n",
    "}\n",
    "\n",
    "prompt = \"Ask a question which is related to the following text, but cannot be answered based on the text. Text: {context}\"\n",
    "context = \"Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\"\n",
    "expanded_prompt = prompt.replace(\"{context}\", context)\n",
    "\n",
    "finetuned_predictor.serializer = serializers.JSONSerializer()\n",
    "finetuned_predictor.content_type = \"application/json\"\n",
    "payload = {\"text_inputs\": expanded_prompt, **parameters}  # JSON Input format\n",
    "\n",
    "# query_response = finetuned_predictor.predict(json.dumps(payload).encode(\"utf-8\"))\n",
    "query_response = finetuned_predictor.predict(payload)\n",
    "print(query_response[\"generated_texts\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beafe5b-07d9-4095-9f4e-7f75b4104c95",
   "metadata": {},
   "source": [
    "### 5. Cleaning up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99076e38-16db-4b77-9545-d21796af4ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2c5d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
